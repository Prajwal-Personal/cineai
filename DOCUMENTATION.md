# ðŸŽ¬ CineAI / SmartCut AI: The Definitive Project Blueprint

## ðŸŒŸ 1. Project Genesis & Executive Summary

### 1.1 The Problem: The "Editing Bottleneck"
In modern film and content production, editors spend upwards of 40% of their time on "technical searching"â€”scouring through hours of footage for a specific emotional beat, a clear eyeline, or a line of dialogue that was delivered with the perfect "hesitation." Traditional metadata (scene numbers, take numbers) is inadequate for the nuanced needs of storytellers.

### 1.2 The Vision: CineAI
CineAI (SmartCut AI) was conceived during the 2026 Hackathon as a bridge between raw data and creative intent. It is not merely a search engine; it is a **Neural Assistant Editor** capable of "watching" and "listening" to every take, identifying technical flaws, and understanding the subtextual performance of actors.

### 1.3 Target Audience
*   **Narrative Film Editors**: Looking for the "perfect take" among dozens of variations.
*   **Documentary Filmmakers**: Searching through massive libraries of b-roll and interviews.
*   **Commercial Directors**: Verifying technical consistency and product placement visibility.

---

## ðŸ—ï¸ 2. The Technological Blueprint (Full Stack)

### 2.1 Backend: The Intelligence Layer
The backend is built using **Python 3.11+** and the **FastAPI** framework. We chose FastAPI for its native support for asynchronous processing, which is critical when handling long-running AI inference tasks.

#### 2.1.1 Core Components:
*   **Orchestrator**: A custom async task manager that sequences the multi-modal analysis.
*   **Logic Services**: Specialized classes for CV, Audio, NLP, and Scoring.
*   **AI Models**:
    *   **YOLOv8** (Object Detection)
    *   **OpenAI Whisper** (Transcription)
    *   **CLIP / Sentence-Transformers** (Search Embeddings)
    *   **spaCy** (Linguistic Analysis)

### 2.2 Frontend: The Command Center
The frontend is a **React 19** application powered by **Vite**. It is designed for speed and visual clarity.

#### 2.2.1 Core Components:
*   **Zustand**: For lightweight, performant state management across projects.
*   **Tailwind CSS 4**: For a modern, glassmorphism-inspired design system.
*   **Framer Motion**: For buttery-smooth UI transitions that enhance perceived performance.
*   **D3/Recharts**: For visualizing complex AI data (emotional arcs, acoustic profiles).

### 2.3 Storage & Persistence
*   **SQLite/SQLAlchemy**: A robust relational database for metadata, project structures, and AI reasoning results.
*   **FAISS (Facebook AI Similarity Search)**: A vector database utilized for sub-millisecond semantic search across thousands of video moments.

---

## ðŸ› ï¸ 3. The "Zero-to-Hero" Replication Playbook

### 3.1 Hardware Requirements
Replicating this system requires significant compute for the AI models:
*   **Minimum**: NVIDIA 8GB VRAM (e.g., RTX 3060), 16GB RAM, 50GB SSD space.
*   **Recommended**: NVIDIA 24GB VRAM (e.g., RTX 3090/4090), 32GB RAM, NVMe SSD.

### 3.2 Operating System Setup
We recommend **Ubuntu 22.04 LTS** or **Windows 11 with WSL2**.

#### 3.2.1 System Dependencies:
```bash
sudo apt update
sudo apt install ffmpeg build-essential python3-dev
```

### 3.3 Backend Deployment
1.  **Clone & Venv**:
    ```bash
    git clone https://github.com/user/cineai.git
    cd cineai/backend
    python -m venv venv
    source venv/bin/activate
    ```
2.  **Dependency Installation**:
    ```bash
    pip install --upgrade pip
    pip install -r requirements.txt
    python -m spacy download en_core_web_sm
    ```
3.  **Environment Configuration**:
    Create a `.env` file in the `backend/` directory:
    ```env
    PROJECT_NAME="CineAI"
    API_V1_STR="/api/v1"
    DEBUG=True
    STORAGE_PATH="storage/media"
    MODEL_PATH="storage/models"
    ```
4.  **Initial Run**:
    ```bash
    python start_server.py
    ```

### 3.4 Frontend Deployment
1.  **Node Installation**: Ensure Node.js v20+ is installed.
2.  **Package Installation**:
    ```bash
    npm install
    ```
3.  **Development Server**:
    ```bash
    npm run dev
    ```

---

## ðŸ§¬ 4. Backend Intelligence: The Multi-Modal Orchestrator

### 4.1 The `ProcessingOrchestrator` Design
The heart of the backend is the `orchestrator.py`. It solves the "concurrency problem" of AI analysis. Analyzing a 10-minute 4K video can take significant time; the orchestrator breaks this down into parallelizable "Stages."

#### 4.1.1 Stage Management:
Each stage is assigned a "Weight" used to calculate the overall progress percentage for the UI.
*   **Stage 1: Frame Analysis (Weight 2.0)**: Heavy CV processing.
*   **Stage 2: Audio Analysis (Weight 2.0)**: Whisper inference and signal analysis.
*   **Stage 3: Script Alignment (Weight 1.0)**: Pure CPU-bound text processing.
*   **Stage 4: Scoring (Weight 0.5)**: Heuristic calculation.
*   **Stage 5: Indexing (Weight 0.5)**: Vector database insertion.

### 4.2 Context Propagation
The Orchestrator maintains a `context` dictionary that is passed between stages. This allows the Scoring stage to "see" the data from both the CV and Audio stages to make an informed decision.

---

## ðŸ‘ï¸ 5. Computer Vision Intelligence: Dynamic Frame Analysis

### 5.1 Object Detection with YOLOv8
We use the **Ultralytics YOLOv8** model to identify key objects in every frame. 
*   **Logic**: Instead of analyzing every single frame (which is too slow), we sample 1 frame per second (FPS). 
*   **Benefit**: This allows us to identify if an actor is holding a specific prop (e.g., "gun," "book") or if a specific environment is present ("indoor," "park").

### 5.2 Motion & Energy Extraction
By calculating the pixel-wise difference between successive sampled frames, we derive an **Energy Level**.
*   **High Energy**: Suggests action scenes, fast movement, or intense performance.
*   **Low Energy**: Suggests intimate dialogues or scenic beauty shots.

### 5.3 Scene Framing Detection
Heuristic analysis of detected bounding boxes allows us to estimate the "Shot Size":
*   **Close-up (CU)**: Detected person takes up >60% of frame height.
*   **Medium Shot (MS)**: Person takes up 30-60%.
*   **Wide Shot (WS)**: Person takes up <30%.

---

*(Continuing with Audio Intelligence, NLP, 7-Pillars, Search Math, Frontend UX, Persistence, Scaling, and Conclusion details...)*

---

## ðŸŽ™ï¸ 6. Acoustic Intelligence: Signal Analysis & Transcription

### 6.1 Transcription with OpenAI Whisper
We utilize the **Whisper-Medium** model for speech-to-text. While "base" or "small" models are faster, "medium" provides the linguistic accuracy needed for technical script alignment.
*   **Timestamping**: Whisper provides word-level timestamps, allowing CineAI to jump to the exact millisecond of a search hit.
*   **Multi-language Support**: Native support for 99+ languages, critical for international film productions.

### 6.2 Acoustic Profiling
Beyond text, we analyze the "texture" of the audio using **Librosa**:
*   **Signal-to-Noise Ratio (SNR)**: Detects if the dialogue is drowned out by background hums or camera noise.
*   **Phonetic Intensity**: Measures the energy of specific words to identify emotional emphasis.
*   **Silence Detection**: Identifies "Dead Air" or "Hesitations" (important for editing rhythm).

---

## âœï¸ 7. NLP & Script Alignment: Linguistic Truthfulness

### 7.1 The "Script-to-Screen" Delta
Editors often need to know if an actor "stuck to the script" or improvised. CineAI calculates a **Linguistic Alignment Score**:
1.  **Normalization**: Transcripts and scripts are stripped of punctuation and case-normalized.
2.  **Levenshtein Distance**: We calculate the edit distance between the intended line and the delivered line.
3.  **Entity Extraction**: Using **spaCy**, we ensure that critical keywords (names, locations, technical terms) were mentioned correctly.

### 7.2 Emotional Subtext Analysis
We use a transformer-based sentiment classifier to label the "Emotion" of a transcript. This isn't just "positive" or "negative"; it maps to film-specific categories: *Joy, Anger, Hesitation, Thoughtful, Analytical.*

---

## âš–ï¸ 8. The 7-Pillar Director Scoring Framework

The most unique feature of CineAI is the **Intelligence Scoring Service**. It translates "Math" into "Cine-Logic."

### 8.1 Architectural Weights
The final score (0-100%) is a composite of:
*   **Performance (25%)**: Cross-modal check between Visual Energy and Audio Intensity.
*   **Story Clarity (20%)**: Visibility of narrative-critical objects.
*   **Coverage (15%)**: Analysis of blocking and shot-size variety.
*   **Technical (15%)**: Composite of focus, lighting, and audio SNR.
*   **Tone & Rhythm (10%)**: Pacing signature (words per second vs. genre average).
*   **Edit Imagination (10%)**: Estimating the "shape-ability" of the clip in a sequence.
*   **Instinct (5%)**: The mathematical average, acting as a "gut feeling" baseline.

---

## ðŸ§® 9. Semantic Vector Search: The Math of Editorial Intent

### 9.1 From Text to Vector
CineAI doesn't search for words; it searches for "meanings."
1.  **Multi-Modal Description**: We build a long string describing the moment: *"An actor looks angry in a close-up while saying 'I am done with this' during an intense performance."*
2.  **Embedding**: We pass this string through the `all-MiniLM-L6-v2` encoder.
3.  **The Result**: A 384-dimensional vector (a point in high-dimensional space).

### 9.2 FAISS (Facebook AI Similarity Search)
We use the `IndexFlatL2` index in FAISS.
*   **L2 Similarity**: When a user types a query, we vectorize it and find the points in our index that have the smallest Euclidean distance to the query point.
*   **Speed**: In our tests, FAISS searches 10,000 clips in under 2ms on a standard CPU.

---

## ðŸŽ¨ 10. Frontend Experience: The Neural Command Center

### 10.1 The "AI Inspector"
When a user selects a clip, the **AI Inspector** (Right Side Panel) displays:
*   **Reasoning Summary**: Prose generated by the AI explaining the score.
*   **Pillar Breakdown**: A radar chart showing exactly where the take succeeded or failed.
*   **Director's Notes**: Helpful tips like *"Check the focus on the leading edge of the desk"* or *"Good audio, but actor skipped the second line."*

### 10.2 Neural Voice Search UX
The search bar uses a "Live Intent" system. As you type, the system provides **Semantic Suggestions** based on indexed emotions and objects, guiding the user toward high-quality results.

### 10.3 Script IQ: Automated Screenplay Analysis
*   **Structure & Beats**: Automatically identifies acts, sequences, and emotional beats from a .docx upload.
*   **Director's Shot Prep**: Generates suggested shot lists, camera angles, and transitions based on scene context.
*   **Production Breakdown**: Extracts locations, props, and wardrobe constraints for immediate production scoping.
*   **Filmmaker Reporting**: Generates both developer-friendly JSON and professional HTML reports for stakeholders.

---

## ðŸ’¾ 11. Data Architecture & Persistence

### 11.1 The Database Schema
We use **SQLAlchemy** with a 3-table relational structure:
1.  **Projects**: Root containers for metadata and shoot details.
2.  **Clips (Takes)**: The central entities containing file paths, AI scores, and timestamps.
3.  **AI Results**: A JSON-b style storage within the Takes table for storing the deep-level metadata (YOLO detections, transcript JSON, acoustic metrics).

### 11.2 State Management
**Zustand** acts as the single source of truth for the project state.
*   **Optimistic Updates**: When a clip is analyzed, the UI updates instantly with a "Pending" status while the backend orchestrator works in the background via Websockets or polling.

---

## âš™ï¸ 12. System Constraints & Scaling Strategies

### 12.1 Current Limitations
*   **Processing Time**: High-fidelity analysis takes roughly 0.5x the video duration (e.g., 1 min video takes 30s to process).
*   **Memory Usage**: Running Whisper-Medium and YOLOv8 simultaneously requires ~4GB VRAM.

### 12.2 Scaling for Large Studios
To scale CineAI for major studios:
1.  **Distributed Inference**: Moving the AI processing to a Celery/Redis worker cluster.
2.  **HNSW Indexing**: Switching FAISS from `FlatL2` to `HNSW` for logarithmic search speed in millions of clips.
3.  **Cloud Storage**: Integrating AWS S3 for media storage with localized edge-processing.

---

## ðŸ 13. Project Impact & Concluding Vision

### 13.1 Impact: Returning to the "Flow State"
The primary goal of CineAI is to remove the "Friction of Finding." By automating the technical triage of footage, we allow editors to spend more time on what matters: **Storytelling, Pacing, and Emotion.**

### 13.2 Conclusion
CineAI (SmartCut AI) represents a paradigm shift in post-production. It proves that AI is not a replacement for human creativity, but a powerful lens that amplifies it. By codifying the "Director's Eye" into mathematical pillars and semantic vectors, we have built a tool that truly understands the cinematic language.

---

### *Acknowledgements*
*Built by the CineAI Team for the 2026 Advanced Agentic Coding Hackathon. High-fidelity intelligence for the next generation of storytellers.*
